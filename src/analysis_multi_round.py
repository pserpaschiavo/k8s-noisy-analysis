#!/usr/bin/env python3
"""
Module: analysis_multi_round.py
Description: M√≥dulo para an√°lise de experimentos com m√∫ltiplos rounds.

Este m√≥dulo implementa metodologias para an√°lise de consist√™ncia entre rounds,
avalia√ß√£o de robustez causal, an√°lise de diverg√™ncia comportamental e
agrega√ß√£o de consenso para experimentos com m√∫ltiplos rounds.
"""

import os
import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx  # Adicionando importa√ß√£o do NetworkX
from typing import Dict, List, Tuple, Any, Optional, Union
from scipy import stats
from scipy.spatial import distance
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from datetime import datetime

from src.pipeline_stage import PipelineStage
from src.visualization.plots import (
    generate_consolidated_heatmap,
    generate_all_enhanced_consolidated_boxplots
)
from src.visualization.advanced_plots import (
    generate_all_consolidated_timeseries,
    generate_consolidated_timeseries,
    plot_aggregated_correlation_graph
)
from src.analysis_correlation import compute_aggregated_correlation
from src.effect_size import extract_effect_sizes  # Importar o novo m√≥dulo
from src.effect_aggregation import aggregate_effect_sizes  # Importar a fun√ß√£o de agrega√ß√£o de efeitos
from src.phase_correlation import extract_phase_correlations, analyze_correlation_stability  # Importar fun√ß√µes de correla√ß√£o intra-fase
from src.visualization.effect_plots import (
    generate_effect_size_heatmap,
    plot_effect_error_bars,
    plot_effect_scatter,
    generate_effect_forest_plot
)
from src.visualization.correlation_plots import (
    plot_correlation_heatmap,
    plot_correlation_network,
    plot_correlation_stability
)
from src.robustness_analysis import perform_robustness_analysis, generate_robustness_summary
from src.insights_generation import generate_automated_insights, generate_markdown_report

# Configura√ß√£o de logging e estilo
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("analysis_multi_round")
plt.style.use('tableau-colorblind10')

class MultiRoundAnalysisStage(PipelineStage):
    """
    Est√°gio do pipeline para an√°lise de experimentos com m√∫ltiplos rounds.
    Implementa as funcionalidades descritas na se√ß√£o 3.6 do plano de trabalho.
    
    Estrutura de visualiza√ß√µes geradas:
    - effect_visualizations/: Heatmaps e gr√°ficos de tamanhos de efeito
    - causality_robustness/: Visualiza√ß√µes de robustez das rela√ß√µes causais
    - correlation_graphs/: Redes de correla√ß√£o entre tenants
    - aggregated_te/: Matrizes de Transfer Entropy agregadas
    - insights/: Relat√≥rios e visualiza√ß√µes de insights autom√°ticos
    - Arquivos individuais: cv_heatmap_by_tenant_metric.png, tenant_stability_scores.png,
      te_consistency_heatmap.png, granger_consistency_heatmap.png
    """
    
    def __init__(self, output_dir: Optional[str] = None):
        super().__init__(
            name="Multi-Round Analysis", 
            description="An√°lise de consist√™ncia e robustez entre m√∫ltiplos rounds de experimento"
        )
        self.output_dir = output_dir
        # Adicionando um logger espec√≠fico para esta classe para melhor rastreabilidade
        self.logger = logging.getLogger(self.__class__.__name__)

    def _execute_implementation(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Implementation of the abstract method from PipelineStage.
        Bridges to the existing run method by extracting config from context.
        """
        # Extract config from context - pipeline stages should include their config
        config = context.get('config', {})
        return self.run(context, config)

    def run(self, context: Dict[str, Any], config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Executa a an√°lise multi-round, combinando consist√™ncia de m√©tricas,
        robustez causal e an√°lise de diverg√™ncia.
        """
        self.logger.info("Iniciando a an√°lise multi-round completa...")

        if 'error' in context:
            self.logger.error(f"Erro em est√°gio anterior: {context['error']}")
            return context

        df_long = context.get('df_long')
        if df_long is None:
            self.logger.warning("DataFrame principal (df_long) n√£o encontrado no contexto.")
            context['error'] = "DataFrame principal n√£o dispon√≠vel para an√°lise multi-round"
            return context

        rounds = config.get('selected_rounds', sorted(df_long['round_id'].unique()))
        if len(rounds) <= 1:
            self.logger.info("Apenas um round encontrado ou selecionado. Pulando an√°lise multi-round.")
            context['multi_round_analysis'] = {
                'status': 'skipped',
                'reason': 'O dataset cont√©m apenas um round de experimento ou apenas um foi selecionado.'
            }
            return context

        # Configura√ß√£o do diret√≥rio de sa√≠da
        # J√° recebemos um diret√≥rio de sa√≠da configurado para multi_round_analysis
        # N√£o precisamos adicionar 'multi_round_analysis' novamente
        base_output_dir = self.output_dir or context.get('output_dir')
        if not base_output_dir:
            experiment_id = context.get('experiment_id', 'unknown_experiment')
            base_output_dir = os.path.join(os.getcwd(), 'outputs', experiment_id)
            self.logger.warning(f"Diret√≥rio de sa√≠da n√£o especificado. Usando fallback: {base_output_dir}")

        # Garantir que o diret√≥rio de sa√≠da existe
        os.makedirs(base_output_dir, exist_ok=True)
        self.logger.info(f"Diret√≥rio de sa√≠da para an√°lise multi-round: {base_output_dir}")
        
        # Atualiza o output_dir da inst√¢ncia para que outros m√©todos o utilizem
        self.output_dir = base_output_dir

        metrics = context.get('selected_metrics', sorted(df_long['metric_name'].unique()))
        tenants = context.get('selected_tenants', sorted(df_long['tenant_id'].unique()))
        
        df_filtered = df_long[df_long['round_id'].isin(rounds)]

        try:
            results = {}

            # 0. Extra√ß√£o de tamanhos de efeito para todos os rounds
            self.logger.info("Realizando extra√ß√£o de tamanhos de efeito em todas as combina√ß√µes...")
            effect_sizes_df = self._extract_effect_sizes(context, config, df_filtered, rounds)
            if not effect_sizes_df.empty:
                results['effect_sizes'] = {
                    'dataframe': effect_sizes_df,
                    'summary': {
                        'total_comparisons': effect_sizes_df.shape[0],
                        'significant_effects': effect_sizes_df[effect_sizes_df['p_value'] < 0.05].shape[0],
                        'large_effects': effect_sizes_df[effect_sizes_df['effect_size'].abs() > 0.8].shape[0],
                        'medium_effects': effect_sizes_df[(effect_sizes_df['effect_size'].abs() > 0.5) & 
                                                         (effect_sizes_df['effect_size'].abs() <= 0.8)].shape[0],
                        'small_effects': effect_sizes_df[(effect_sizes_df['effect_size'].abs() > 0.2) & 
                                                        (effect_sizes_df['effect_size'].abs() <= 0.5)].shape[0]
                    }
                }
                
                # 0.1 Agrega√ß√£o de tamanhos de efeito entre rounds
                self.logger.info("Agregando tamanhos de efeito entre rounds...")
                aggregated_effects_df = self._aggregate_effect_sizes(effect_sizes_df, config)
                if not aggregated_effects_df.empty:
                    results['aggregated_effects'] = {
                        'dataframe': aggregated_effects_df,
                        'summary': {
                            'total_combinations': aggregated_effects_df.shape[0],
                            'significant_combinations': aggregated_effects_df['is_significant'].sum(),
                            'high_reliability': (aggregated_effects_df['reliability_category'] == 'high').sum(),
                            'medium_reliability': (aggregated_effects_df['reliability_category'] == 'medium').sum(),
                            'low_reliability': (aggregated_effects_df['reliability_category'] == 'low').sum()
                        }
                    }
                    
                    # Realizar an√°lise de robustez dos tamanhos de efeito
                    self.logger.info("Realizando an√°lise de robustez dos tamanhos de efeito...")
                    robustness_output_dir = os.path.join(self.output_dir, 'robustness') if self.output_dir else None
                    
                    robustness_results = perform_robustness_analysis(
                        effect_sizes_df=effect_sizes_df,
                        aggregated_effects_df=aggregated_effects_df,
                        leave_one_out=True,
                        sensitivity_test=True
                    )
                    
                    # Salvar visualiza√ß√µes de robustez se temos resultados
                    if 'robustness_score' in robustness_results:
                        generate_robustness_summary(
                            robustness_results=robustness_results,
                            output_dir=robustness_output_dir
                        )
                    
                    # Armazenar resultados de robustez no dicion√°rio de resultados
                    if isinstance(robustness_results, dict):
                        results['effect_robustness'] = robustness_results
                        
                        # Usar a vers√£o enriquecida do DataFrame para o resto da an√°lise se dispon√≠vel
                        if 'enhanced_aggregated_effects' in robustness_results:
                            enhanced_df = robustness_results['enhanced_aggregated_effects']
                            results['aggregated_effects']['enhanced_df'] = enhanced_df
                        
                        # O relat√≥rio j√° foi gerado pela fun√ß√£o generate_robustness_summary
                        
                    # Gerar visualiza√ß√µes para os tamanhos de efeito agregados
                    self.logger.info("Gerando visualiza√ß√µes para os tamanhos de efeito agregados...")
                    
                    if self.output_dir:
                        effect_viz_dir = os.path.join(self.output_dir, 'effect_visualizations')
                        os.makedirs(effect_viz_dir, exist_ok=True)
                        
                        # Heatmap de tamanhos de efeito
                        heatmap_paths = []
                        for metric in aggregated_effects_df['metric_name'].unique():
                            path = generate_effect_size_heatmap(
                                aggregated_effects_df=aggregated_effects_df,
                                output_dir=effect_viz_dir,
                                metric=metric
                            )
                            if path:
                                heatmap_paths.append(path)
                                
                        # Error bars com IC95%
                        error_bar_paths = []
                        for metric in aggregated_effects_df['metric_name'].unique():
                            path = plot_effect_error_bars(
                                aggregated_effects_df=aggregated_effects_df,
                                output_dir=effect_viz_dir,
                                metric=metric
                            )
                            if path:
                                error_bar_paths.append(path)
                                
                        # Gr√°ficos de dispers√£o
                        scatter_paths = []
                        for metric in aggregated_effects_df['metric_name'].unique():
                            path = plot_effect_scatter(
                                aggregated_effects_df=aggregated_effects_df,
                                output_dir=effect_viz_dir,
                                metric=metric
                            )
                            if path:
                                scatter_paths.append(path)
                                
                        # Forest plots para visualiza√ß√£o meta-an√°lise style
                        forest_paths = []
                        # Gerar forest plots para cada m√©trica √ó fase √ó tenant
                        # Aqui limitamos para as combina√ß√µes mais importantes para n√£o gerar gr√°ficos demais
                        important_metrics = aggregated_effects_df.sort_values(
                            by='mean_effect_size', key=abs, ascending=False
                        )['metric_name'].unique()[:5]  # Top 5 m√©tricas com maior efeito
                        
                        important_tenants = aggregated_effects_df.sort_values(
                            by='mean_effect_size', key=abs, ascending=False
                        )['tenant_id'].unique()[:3]  # Top 3 tenants com maior efeito
                        
                        for metric in important_metrics:
                            for tenant in important_tenants:
                                paths = generate_effect_forest_plot(
                                    effect_sizes_df=effect_sizes_df,
                                    aggregated_effects_df=aggregated_effects_df,
                                    output_dir=effect_viz_dir,
                                    metric=metric,
                                    tenant=tenant,
                                    show_reliability_indicator=True,
                                    sort_by='effect_size'
                                )
                                
                                if isinstance(paths, list):
                                    forest_paths.extend(paths)
                                elif paths:
                                    forest_paths.append(paths)
                                
                        results['aggregated_effects']['visualizations'] = {
                            'heatmaps': heatmap_paths,
                            'error_bars': error_bar_paths,
                            'scatter_plots': scatter_paths,
                            'forest_plots': forest_paths
                        }
                        
                        # Gerar visualiza√ß√µes de s√©ries temporais consolidadas
                        self.logger.info("Gerando visualiza√ß√µes de s√©ries temporais consolidadas...")
                        timeseries_dir = os.path.join(self.output_dir, 'timeseries')
                        os.makedirs(timeseries_dir, exist_ok=True)
                        
                        # Filtrar m√©tricas importantes (as mesmas usadas para forest plots)
                        important_metrics = aggregated_effects_df.sort_values(
                            by='mean_effect_size', key=abs, ascending=False
                        )['metric_name'].unique()[:5]  # Top 5 m√©tricas com maior efeito
                        
                        # Gerar visualiza√ß√µes de time series consolidadas
                        consolidated_timeseries_results = {}
                        for metric in important_metrics:
                            try:
                                output_paths = generate_consolidated_timeseries(
                                    df_long=df_filtered,
                                    metric=metric,
                                    output_dir=timeseries_dir,
                                    cross_tenant_comparison=True,
                                    create_animations=True
                                )
                                if output_paths:
                                    consolidated_timeseries_results[metric] = output_paths
                                    self.logger.info(f"‚úÖ Time series consolidados gerados para {metric}")
                                else:
                                    self.logger.warning(f"‚ùå N√£o foi poss√≠vel gerar time series para {metric}")
                            except Exception as e:
                                self.logger.error(f"‚ùå Erro ao processar time series para {metric}: {e}", exc_info=True)
                        
                        if consolidated_timeseries_results:
                            results['consolidated_timeseries'] = consolidated_timeseries_results
                    
            # 0.2 Extra√ß√£o de correla√ß√µes intra-fase
            self.logger.info("Realizando extra√ß√£o de correla√ß√µes intra-fase entre tenants...")
            phase_correlations_df = self._extract_phase_correlations(context, config, df_filtered, rounds)
            if not phase_correlations_df.empty:
                results['phase_correlations'] = {
                    'dataframe': phase_correlations_df,
                    'summary': {
                        'total_correlations': phase_correlations_df.shape[0],
                        'strong_correlations': (phase_correlations_df['correlation_strength'] == 'strong').sum(),
                        'moderate_correlations': (phase_correlations_df['correlation_strength'] == 'moderate').sum(),
                        'weak_correlations': (phase_correlations_df['correlation_strength'] == 'weak').sum(),
                        'positive_correlations': (phase_correlations_df['correlation'] > 0).sum(),
                        'negative_correlations': (phase_correlations_df['correlation'] < 0).sum()
                    }
                }
                
                # Analisar estabilidade das correla√ß√µes intra-fase
                self.logger.info("Analisando estabilidade das correla√ß√µes intra-fase...")
                correlation_stability_df = analyze_correlation_stability(phase_correlations_df)
                
                if isinstance(correlation_stability_df, pd.DataFrame) and not correlation_stability_df.empty:
                    results['phase_correlations']['stability'] = {
                        'dataframe': correlation_stability_df,
                        'summary': {
                            'total_analyzed': correlation_stability_df.shape[0],
                            'high_stability': len(correlation_stability_df[correlation_stability_df['stability_category'] == 'high']),
                            'medium_stability': len(correlation_stability_df[correlation_stability_df['stability_category'] == 'medium']),
                            'low_stability': len(correlation_stability_df[correlation_stability_df['stability_category'] == 'low'])
                        }
                    }
                    
                    # Gerar visualiza√ß√µes para as correla√ß√µes intra-fase
                    self.logger.info("Gerando visualiza√ß√µes para correla√ß√µes intra-fase...")
                    
                    if self.output_dir:
                        correlation_viz_dir = os.path.join(self.output_dir, 'correlation_visualizations')
                        os.makedirs(correlation_viz_dir, exist_ok=True)
                        
                        # 1. Heatmaps de correla√ß√£o para cada combina√ß√£o de m√©trica, fase e round
                        heatmap_paths = []
                        for metric in phase_correlations_df['metric_name'].unique():
                            for phase in phase_correlations_df['experimental_phase'].unique():
                                for round_id in phase_correlations_df['round_id'].unique():
                                    path = plot_correlation_heatmap(
                                        correlation_df=phase_correlations_df,
                                        output_dir=correlation_viz_dir,
                                        metric=metric,
                                        phase=phase,
                                        round_id=round_id
                                    )
                                    if path:
                                        heatmap_paths.append(path)
                        
                        # 2. Redes de correla√ß√£o para cada combina√ß√£o de m√©trica e fase
                        network_paths = []
                        for metric in phase_correlations_df['metric_name'].unique():
                            for phase in phase_correlations_df['experimental_phase'].unique():
                                path = plot_correlation_network(
                                    correlation_df=phase_correlations_df,
                                    output_dir=correlation_viz_dir,
                                    metric=metric,
                                    phase=phase
                                )
                                if path:
                                    network_paths.append(path)
                        
                        # 3. Visualiza√ß√µes de estabilidade para cada combina√ß√£o de m√©trica e fase
                        stability_paths = []
                        for metric in correlation_stability_df['metric_name'].unique():
                            for phase in correlation_stability_df['experimental_phase'].unique():
                                path = plot_correlation_stability(
                                    correlation_df=phase_correlations_df,
                                    stability_df=results['phase_correlations']['stability']['dataframe'],
                                    output_dir=correlation_viz_dir,
                                    metric=metric,
                                    phase=phase
                                )
                                if path:
                                    stability_paths.append(path)
                        
                        results['phase_correlations']['visualizations'] = {
                            'heatmaps': heatmap_paths,
                            'networks': network_paths,
                            'stability_plots': stability_paths
                        }

            # 1. An√°lise de consist√™ncia de causalidade (Jaccard/Spearman)
            self.logger.info("Analisando a consist√™ncia da estrutura causal (Jaccard/Spearman)...")
            causality_data = self._load_causality_matrices(context, rounds)
            te_matrices_by_round = causality_data.get('te_matrices_by_round', {})
            granger_matrices_by_round = causality_data.get('granger_matrices_by_round', {})
            
            # Reformatar dados para o m√©todo de an√°lise de consist√™ncia
            causality_matrices_reformatted = {}
            for r in rounds:
                if r in te_matrices_by_round or r in granger_matrices_by_round:
                     causality_matrices_reformatted[r] = {
                         'te_matrices': te_matrices_by_round.get(r, {}),
                         'granger_matrices': granger_matrices_by_round.get(r, {})
                     }

            if causality_matrices_reformatted:
                graph_consistency_results = self.analyze_causality_consistency(causality_matrices_reformatted)
                results['graph_consistency'] = graph_consistency_results
                self.generate_consistency_visualizations(graph_consistency_results)
            else:
                self.logger.warning("Nenhuma matriz de causalidade carregada. Pulando an√°lise de consist√™ncia da estrutura causal.")
                results['graph_consistency'] = {}


            # 2. An√°lise de consist√™ncia de m√©tricas (CV/Friedman)
            self.logger.info("Analisando a consist√™ncia dos valores das m√©tricas (CV/Friedman)...")
            results['metric_consistency'] = analyze_round_consistency(
                df_long=df_filtered, metrics=metrics, tenants=tenants, output_dir=self.output_dir
            )

            # 3. An√°lise de robustez de causalidade (CV sobre TE)
            if te_matrices_by_round:
                self.logger.info("Analisando a robustez da for√ßa causal (CV sobre TE)...")
                # Use the function defined in this module
                causality_robustness = analyze_causality_robustness(
                    te_matrices_by_round=te_matrices_by_round,
                    granger_matrices_by_round=granger_matrices_by_round,
                    output_dir=self.output_dir
                )
                results['causality_robustness'] = causality_robustness

                if 'robust_causal_relationships' in causality_robustness:
                    # Use the function defined below
                    def generate_robust_causality_graph(robust_relationships, output_dir, metric):
                        """
                        Generate a graph visualization of robust causal relationships for a metric.
                        
                        Args:
                            robust_relationships: Dictionary of robust causal relationships.
                            output_dir: Directory to save the graph.
                            metric: Metric to generate the graph for.
                            
                        Returns:
                            Path to the generated graph image.
                        """
                        try:
                            if metric not in robust_relationships or not robust_relationships[metric]:
                                return None
                                
                            relationships = robust_relationships[metric]
                            if not relationships:
                                return None
                                
                            # Create directed graph
                            G = nx.DiGraph()
                            
                            # Add nodes and edges
                            nodes = set()
                            for (source, target), data in relationships.items():
                                nodes.add(source)
                                nodes.add(target)
                                # Use mean TE as edge weight
                                G.add_edge(source, target, weight=data['mean_te'])
                            
                            # If no edges, return None
                            if not G.edges():
                                return None
                                
                            # Add nodes
                            for node in nodes:
                                G.add_node(node)
                                
                            plt.figure(figsize=(10, 8))
                            
                            # Node positioning
                            pos = nx.spring_layout(G, seed=42)
                            
                            # Draw nodes
                            nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue')
                            
                            # Draw edges with width proportional to weight (TE)
                            edges = list(G.edges())
                            edge_weights = [G[u][v]['weight'] * 10 for u, v in edges]
                            nx.draw_networkx_edges(G, pos, edgelist=edges, width=1.0, arrowsize=20, alpha=0.7)
                            
                            # Add labels
                            nx.draw_networkx_labels(G, pos)
                            
                            # Add edge labels (TE values)
                            edge_labels = {(u, v): f"{G[u][v]['weight']:.3f}" for u, v in G.edges()}
                            nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)
                            
                            plt.title(f'Robust Causal Relationships: {metric}')
                            plt.axis('off')
                            plt.tight_layout()
                            
                            # Save figure
                            os.makedirs(output_dir, exist_ok=True)
                            graph_path = os.path.join(output_dir, f'robust_causality_graph_{metric}.png')
                            plt.savefig(graph_path, dpi=300, bbox_inches='tight')
                            plt.close()
                            
                            return graph_path
                        except Exception as e:
                            logger.warning(f"Error generating causality graph for {metric}: {str(e)}")
                            return None
                    
                    # Generate graphs
                    robust_graphs = {}
                    for metric in causality_robustness.get('metrics_with_consistent_causality', []):
                        graph_path = generate_robust_causality_graph(
                            robust_relationships=causality_robustness['robust_causal_relationships'],
                            output_dir=self.output_dir, metric=metric
                        )
                        if graph_path:
                            robust_graphs[metric] = graph_path
                    if robust_graphs:
                        results['robust_causality_graphs'] = robust_graphs
            else:
                self.logger.warning("Matrizes de Transfer Entropy n√£o dispon√≠veis. Pulando an√°lise de robustez causal.")

            # 4. An√°lise de diverg√™ncia comportamental (KL-Divergence)
            self.logger.info("Analisando a diverg√™ncia comportamental entre rounds (KL-Divergence)...")
            results['behavioral_divergence'] = analyze_behavioral_divergence(
                df_long=df_filtered, metrics=metrics, tenants=tenants, output_dir=self.output_dir
            )

            # 5. Agrega√ß√£o de consenso
            if te_matrices_by_round:
                self.logger.info("Agregando um consenso entre os rounds...")
                results['consensus'] = aggregate_round_consensus(
                    df_long=df_filtered,
                    te_matrices_by_round=te_matrices_by_round,
                    consistency_results=results.get('metric_consistency', {}),
                    output_dir=self.output_dir
                )
            else:
                 self.logger.warning("Matrizes de Transfer Entropy n√£o dispon√≠veis. Pulando agrega√ß√£o de consenso.")

            # 6. Gera√ß√£o de insights autom√°ticos
            self.logger.info("Gerando insights autom√°ticos...")
            try:
                # Determinar quais DataFrames est√£o dispon√≠veis para insights
                effect_sizes_available = 'effect_sizes' in results and 'dataframe' in results['effect_sizes']
                aggregated_effects_available = 'aggregated_effects' in results and 'dataframe' in results['aggregated_effects']
                robustness_available = 'effect_robustness' in results and 'dataframe' in results['effect_robustness']
                correlations_available = 'phase_correlations' in results and 'dataframe' in results['phase_correlations']
                correlation_stability_available = correlations_available and 'stability' in results['phase_correlations']
                
                # Gerar insights apenas se houver dados agregados dispon√≠veis
                if aggregated_effects_available:
                    aggregated_effects_df = results['aggregated_effects']['dataframe']
                    robustness_df = results['effect_robustness']['dataframe'] if robustness_available else None
                    phase_correlations_df = results['phase_correlations']['dataframe'] if correlations_available else None
                    correlation_stability_df = results['phase_correlations']['stability']['dataframe'] if correlation_stability_available else None
                    
                    # Gerar insights
                    # Extrair fases experimentais dos dados para o contexto
                    all_phases = sorted(aggregated_effects_df['experimental_phase'].unique().tolist())
                    baseline_phases = aggregated_effects_df['baseline_phase'].unique().tolist()
                    experimental_phases = [phase for phase in all_phases if phase not in baseline_phases]
                    
                    insights = generate_automated_insights(
                        aggregated_effects_df=aggregated_effects_df,
                        robustness_df=robustness_df,
                        phase_correlations_df=phase_correlations_df,
                        correlation_stability_df=correlation_stability_df,
                        context={
                            'experiment_name': context.get('experiment_name', 'An√°lise Multi-round'),
                            'rounds': rounds,
                            'metrics': metrics,
                            'phases': experimental_phases,  # Agora definido corretamente
                            'tenants': tenants
                        }
                    )
                    
                    # Adicionar insights aos resultados
                    results['automated_insights'] = insights
                    
                    # Gerar relat√≥rio markdown
                    if self.output_dir:
                        insights_dir = os.path.join(self.output_dir, 'insights')
                        os.makedirs(insights_dir, exist_ok=True)
                        report_path = os.path.join(insights_dir, 'automated_insights_report.md')
                        markdown_report = generate_markdown_report(insights, report_path)
                        
                        self.logger.info(f"‚úÖ Insights autom√°ticos gerados e salvos em: {report_path}")
                    else:
                        self.logger.info("‚úÖ Insights autom√°ticos gerados (sem diret√≥rio de sa√≠da definido)")
                else:
                    self.logger.warning("N√£o h√° tamanhos de efeito agregados dispon√≠veis. Pulando gera√ß√£o de insights.")
            except Exception as e:
                self.logger.error(f"Erro ao gerar insights autom√°ticos: {str(e)}")
                import traceback
                self.logger.error(traceback.format_exc())
            
            # 7. Visualiza√ß√µes consolidadas
            self.logger.info("Gerando visualiza√ß√µes consolidadas...")
            
            # Extrair par√¢metros de visualiza√ß√£o da configura√ß√£o
            vis_config = config.get('visualization', {})
            correlation_threshold = vis_config.get('correlation_graph', {}).get('threshold', 0.3)
            causality_threshold = vis_config.get('causality_graph', {}).get('threshold', 0.05)
            
            # Usar os novos par√¢metros configur√°veis
            self.logger.info(f"Usando threshold de correla√ß√£o: {correlation_threshold}")
            
            # Computar as correla√ß√µes agregadas
            try:
                # Importar a fun√ß√£o compute_aggregated_correlation
                from src.analysis_correlation import compute_aggregated_correlation
                
                # Obter a lista de fases dos dados
                phases_list = sorted(df_filtered['experimental_phase'].unique().tolist())
                
                # Calcular correla√ß√µes agregadas para todos os rounds, fases e m√©tricas
                aggregated_correlations = compute_aggregated_correlation(
                    df=df_filtered,
                    metrics=metrics,
                    rounds=rounds,
                    phases=phases_list,
                    method='pearson'
                )
                
                # Plotar gr√°ficos de correla√ß√£o agregada
                for metric in metrics:
                    if metric in aggregated_correlations and not aggregated_correlations[metric].empty:
                        plot_aggregated_correlation_graph(
                            correlation_matrix=aggregated_correlations[metric],
                            title=f"Correla√ß√£o Agregada Multi-round - {metric}",
                            output_dir=os.path.join(self.output_dir, "correlation_graphs"),
                            filename=f"aggregated_correlation_graph_{metric}.png",
                            threshold=correlation_threshold
                        )
                        self.logger.info(f"Gr√°fico de correla√ß√£o agregada gerado para {metric}")
            except Exception as e:
                self.logger.error(f"Erro ao gerar gr√°ficos de correla√ß√£o agregada: {str(e)}")
                import traceback
                self.logger.error(traceback.format_exc())

            # 7. Gerar relat√≥rio consolidado
            self.logger.info("Gerando relat√≥rio consolidado da an√°lise multi-round...")
            self.generate_multi_round_report(results) # Passando todos os resultados

            context['multi_round_analysis'] = results
            context['multi_round_analysis_dir'] = self.output_dir
            self.logger.info(f"An√°lise multi-round conclu√≠da com sucesso. Resultados em {self.output_dir}")

        except Exception as e:
            self.logger.error(f"Erro fatal durante a an√°lise multi-round: {str(e)}", exc_info=True)
            context['error'] = f"Erro na an√°lise multi-round: {str(e)}"

        return context

    def analyze_causality_consistency(self, causality_matrices: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analisa a consist√™ncia das matrizes de causalidade entre os rounds.
        Calcula a similaridade de Jaccard para as rela√ß√µes causais (Granger)
        e a correla√ß√£o de Spearman para a for√ßa da causalidade (TE).
        """
        self.logger.info("Analisando a consist√™ncia da causalidade entre os rounds...")
        
        consistency_results = {
            "granger_jaccard": pd.DataFrame(),
            "te_spearman": pd.DataFrame()
        }
        
        rounds = list(causality_matrices.keys())
        if len(rounds) < 2:
            self.logger.warning("S√£o necess√°rios pelo menos dois rounds para a an√°lise de consist√™ncia.")
            return consistency_results

        # Extrai as matrizes de Granger e TE
        granger_matrices_by_round = {r: v.get('granger_matrices', {}) for r, v in causality_matrices.items()}
        te_matrices_by_round = {r: v.get('te_matrices', {}) for r, v in causality_matrices.items()}

        metrics = list(granger_matrices_by_round[rounds[0]].keys())
        
        jaccard_scores = []
        spearman_scores = []

        for metric in metrics:
            for i in range(len(rounds)):
                for j in range(i + 1, len(rounds)):
                    round1, round2 = rounds[i], rounds[j]
                    
                    # Consist√™ncia para Causalidade de Granger (Jaccard)
                    g1 = granger_matrices_by_round.get(round1, {}).get(metric)
                    g2 = granger_matrices_by_round.get(round2, {}).get(metric)
                    
                    if g1 is not None and g2 is not None:
                        # Binariza a matriz com base no p-valor < 0.05
                        g1_bin = (g1 < 0.05).values.flatten()
                        g2_bin = (g2 < 0.05).values.flatten()
                        
                        jaccard_sim = 1 - distance.jaccard(g1_bin, g2_bin)
                        jaccard_scores.append((metric, f"{round1}-{round2}", jaccard_sim))

                    # Consist√™ncia para Entropia de Transfer√™ncia (Spearman)
                    te1 = te_matrices_by_round.get(round1, {}).get(metric)
                    te2 = te_matrices_by_round.get(round2, {}).get(metric)

                    if te1 is not None and te2 is not None:
                        rho, _ = stats.spearmanr(te1.values.flatten(), te2.values.flatten())
                        spearman_scores.append((metric, f"{round1}-{round2}", rho))

        if jaccard_scores:
            df_jaccard = pd.DataFrame(jaccard_scores, columns=['Metric', 'Round Pair', 'Jaccard Similarity'])
            consistency_results['granger_jaccard'] = df_jaccard.pivot(index='Metric', columns='Round Pair', values='Jaccard Similarity')

        if spearman_scores:
            df_spearman = pd.DataFrame(spearman_scores, columns=['Metric', 'Round Pair', 'Spearman Correlation'])
            # Tratar NaNs que podem surgir se os dados de entrada forem constantes
            df_spearman['Spearman Correlation'] = df_spearman['Spearman Correlation'].fillna(0)
            consistency_results['te_spearman'] = df_spearman.pivot(index='Metric', columns='Round Pair', values='Spearman Correlation')
        else:
            # Garante que a chave exista mesmo que n√£o haja dados
            consistency_results['te_spearman'] = pd.DataFrame()
            
        self.logger.info("An√°lise de consist√™ncia de causalidade conclu√≠da.")
        self.logger.debug(f"Resultados de Jaccard (Granger):\n{consistency_results['granger_jaccard']}")
        self.logger.debug(f"Resultados de Spearman (TE):\n{consistency_results['te_spearman']}")

        return consistency_results

    def generate_consistency_visualizations(self, consistency_results: Dict[str, Any]):
        """
        Gera visualiza√ß√µes para os resultados da an√°lise de consist√™ncia.
        Cria heatmaps para a similaridade de Jaccard (Granger) e correla√ß√£o de Spearman (TE).
        """
        self.logger.info("Gerando visualiza√ß√µes de consist√™ncia...")
        
        if not self.output_dir:
            self.logger.warning("Diret√≥rio de sa√≠da n√£o configurado. As visualiza√ß√µes n√£o ser√£o salvas.")
            return

        # Visualiza√ß√£o para Jaccard (Granger)
        df_jaccard = consistency_results.get('granger_jaccard')
        if df_jaccard is not None and not df_jaccard.empty:
            output_path = generate_consolidated_heatmap(
                aggregated_matrix=df_jaccard,
                output_dir=self.output_dir,
                title="Consist√™ncia da Causalidade de Granger (Similaridade de Jaccard)",
                filename="granger_consistency_heatmap.png"
            )
            if output_path:
                self.logger.info(f"Heatmap de consist√™ncia de Granger salvo em: {output_path}")

        # Visualiza√ß√£o para Spearman (TE)
        df_spearman = consistency_results.get('te_spearman')
        if df_spearman is not None and not df_spearman.empty:
            output_path = generate_consolidated_heatmap(
                aggregated_matrix=df_spearman,
                output_dir=self.output_dir,
                title="Consist√™ncia da For√ßa Causal (Correla√ß√£o de Spearman para TE)",
                filename="te_consistency_heatmap.png"
            )
            if output_path:
                self.logger.info(f"Heatmap de consist√™ncia de TE salvo em: {output_path}")

    def generate_multi_round_report(self, all_results: Dict[str, Any]):
        """
        Gera um relat√≥rio markdown consolidado com todos os resultados da an√°lise multi-round.
        """
        self.logger.info("Gerando relat√≥rio multi-round consolidado...")
        if not self.output_dir:
            self.logger.warning("Diret√≥rio de sa√≠da n√£o configurado. O relat√≥rio n√£o ser√° salvo.")
            return

        report_path = os.path.join(self.output_dir, "multi_round_analysis_report.md")
        
        with open(report_path, "w") as f:
            f.write("# Relat√≥rio Consolidado de An√°lise Multi-Round\n\n")
            f.write(f"Relat√≥rio gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("Este relat√≥rio apresenta uma an√°lise compreensiva de m√∫ltiplos rounds de um experimento, avaliando a consist√™ncia, robustez e diverg√™ncias comportamentais para fornecer um veredito consolidado sobre os resultados.\n\n")

            # --- Se√ß√£o 1: Consist√™ncia da Estrutura Causal (Jaccard/Spearman) ---
            f.write("## 1. Consist√™ncia da Estrutura Causal\n\n")
            f.write("Avalia a consist√™ncia das rela√ß√µes causais identificadas entre os rounds.\n\n")
            
            graph_consistency = all_results.get('graph_consistency', {})
            df_jaccard = graph_consistency.get('granger_jaccard')
            if df_jaccard is not None and not df_jaccard.empty:
                f.write("### 1.1. Causalidade de Granger (Similaridade de Jaccard)\n")
                f.write("A tabela a seguir mostra a similaridade de Jaccard entre os conjuntos de rela√ß√µes causais (p < 0.05) para cada par de rounds. Valores mais pr√≥ximos de 1 indicam maior consist√™ncia na ESTRUTURA do grafo causal.\n\n")
                f.write(df_jaccard.to_markdown())
                f.write("\n\n![Heatmap de Consist√™ncia de Granger](./granger_consistency_heatmap.png)\n\n")

            df_spearman = graph_consistency.get('te_spearman')
            if df_spearman is not None and not df_spearman.empty:
                f.write("### 1.2. For√ßa Causal - Transfer Entropy (Correla√ß√£o de Spearman)\n")
                f.write("A tabela a seguir mostra a correla√ß√£o de Spearman entre as matrizes de Transfer√™ncia de Entropia (TE). Esta m√©trica avalia a consist√™ncia na FOR√áA da causalidade. Valores pr√≥ximos de 1 indicam uma forte correla√ß√£o positiva na for√ßa causal entre os rounds.\n")
                f.write("*Nota: Valores de correla√ß√£o nulos (NaN) foram convertidos para 0, indicando aus√™ncia de correla√ß√£o consistente ou dados de entrada constantes (sem variabilidade na for√ßa causal).*\n\n")
                f.write(df_spearman.to_markdown(floatfmt=".4f"))
                f.write("\n\n![Heatmap de Consist√™ncia de TE](./te_consistency_heatmap.png)\n\n")

            # --- Se√ß√£o 2: Robustez Causal e Grafos Robustos ---
            f.write("## 2. Robustez das Rela√ß√µes Causais\n\n")
            causality_robustness = all_results.get('causality_robustness', {})
            if causality_robustness:
                 f.write("An√°lise da robustez das rela√ß√µes causais individuais com base na sua consist√™ncia (baixo Coeficiente de Varia√ß√£o) atrav√©s dos rounds.\n\n")
                 f.write("### Rela√ß√µes Causais Robustas (Consenso)\n")
                 f.write("Rela√ß√µes que aparecem consistentemente com for√ßa similar em m√∫ltiplos rounds.\n\n")
                 # Link para o CSV
                 f.write("Para uma lista detalhada, veja o arquivo `robust_causal_relationships.csv`.\n\n")
                 
                 robust_graphs = all_results.get('robust_causality_graphs', {})
                 if robust_graphs:
                     f.write("### Grafos de Causalidade Robustos\n")
                     f.write("Grafos mostrando apenas as rela√ß√µes causais mais robustas para m√©tricas selecionadas.\n\n")
                     for metric, path in robust_graphs.items():
                         f.write(f"**M√©trica: {metric}**\n")
                         f.write(f"![Grafo Robusto para {metric}](./{os.path.basename(path)})\n\n")
            else:
                f.write("An√°lise de robustez causal n√£o foi executada ou n√£o produziu resultados.\n\n")


            # --- Se√ß√£o 3: Consist√™ncia de M√©tricas (CV) ---
            f.write("## 3. Consist√™ncia dos Valores de M√©tricas\n\n")
            f.write("An√°lise da estabilidade dos valores das m√©tricas atrav√©s dos rounds, utilizando o Coeficiente de Varia√ß√£o (CV). Baixo CV indica alta consist√™ncia.\n\n")
            f.write("![Heatmap de CV por Tenant e M√©trica](./cv_heatmap_by_tenant_metric.png)\n\n")
            f.write("Para dados detalhados, veja `round_consistency_cv.csv`.\n\n")

            # --- Se√ß√£o 4: Diverg√™ncia Comportamental ---
            f.write("## 4. An√°lise de Diverg√™ncia Comportamental\n\n")
            f.write("Identifica rounds com comportamento an√¥malo e mede a estabilidade do comportamento dos tenants atrav√©s dos rounds usando a Diverg√™ncia de Kullback-Leibniz.\n\n")
            f.write("Para dados detalhados, veja `tenant_stability_scores.csv`.\n\n")

            # --- Se√ß√£o 4.1: Boxplots Consolidados (Aprimorado v2.1) ---
            f.write("## 4.1. Boxplots Consolidados (Violin Plots)\n\n")
            f.write("**üÜï Visualiza√ß√µes aprimoradas** que mostram a distribui√ß√£o de cada m√©trica por fase experimental, agregando dados de todos os rounds. Os violin plots oferecem uma vis√£o mais rica da densidade dos dados em compara√ß√£o com os boxplots tradicionais.\n\n")
            
            consolidated_boxplots = all_results.get('consolidated_boxplots', {})
            if consolidated_boxplots:
                f.write("### Boxplots por M√©trica\n")
                f.write("Para cada m√©trica, s√£o gerados dois gr√°ficos:\n")
                f.write("- **Valores Brutos**: Mostra a distribui√ß√£o real dos dados.\n")
                f.write("- **Valores Normalizados**: Normaliza os dados pela m√©dia da fase 'Baseline' de cada tenant, permitindo uma compara√ß√£o justa do *impacto relativo* das fases de stress.\n\n")
                
                # Organizar por m√©trica para apresentar lado a lado
                metrics_boxplot = sorted(list(set([k.replace('_raw', '').replace('_normalized', '') for k in consolidated_boxplots.keys()])))
                
                for metric in metrics_boxplot:
                    metric_display = metric.replace("_", " ").title()
                    f.write(f"#### {metric_display}\n")
                    
                    raw_path = consolidated_boxplots.get(f"{metric}_raw")
                    norm_path = consolidated_boxplots.get(f"{metric}_normalized")
                    
                    if raw_path:
                        f.write(f"![Boxplot {metric_display}](./boxplots/{os.path.basename(raw_path)})\n")
                    if norm_path:
                        f.write(f"![Boxplot Normalizado {metric_display}](./boxplots/{os.path.basename(norm_path)})\n")
                    f.write("\n")

            else:
                f.write("Boxplots consolidados n√£o foram gerados nesta execu√ß√£o.\n\n")

            # --- Se√ß√£o 4.2: Time Series Consolidados (v2.0) ---
            f.write("## 4.2. Time Series Consolidados\n\n")
            f.write("**Visualiza√ß√µes avan√ßadas** que agregam a evolu√ß√£o temporal de todas as m√©tricas atrav√©s dos rounds, facilitando a identifica√ß√£o de padr√µes, tend√™ncias e diverg√™ncias comportamentais.\n\n")
            
            consolidated_timeseries = all_results.get('consolidated_timeseries', {})
            if consolidated_timeseries:
                f.write("### Time Series por M√©trica\n")
                f.write("Cada visualiza√ß√£o inclui:\n")
                f.write("- **Evolu√ß√£o por Round**: Tend√™ncias agregadas entre todos os tenants\n")
                f.write("- **Evolu√ß√£o por Tenant**: Comportamento individual de cada tenant em todos os rounds\n")
                f.write("- **Tend√™ncias Suavizadas**: M√©dias m√≥veis para identificar padr√µes de longo prazo\n")
                f.write("- **Distribui√ß√µes por Fase**: Boxplots comparando fases experimentais\n\n")
                
                for metric, paths in consolidated_timeseries.items():
                    metric_display = metric.replace("_", " ").title()
                    f.write(f"#### {metric_display}\n")
                    if isinstance(paths, dict):
                        for plot_type, path in paths.items():
                            if path: # Garante que o caminho n√£o √© nulo
                                f.write(f"![{plot_type.replace('_', ' ').title()}](./timeseries/{os.path.basename(path)})\n")
                        f.write("\n")
                    elif isinstance(paths, str): # Fallback para o formato antigo
                         f.write(f"![Time Series Consolidado - {metric_display}](./timeseries/{os.path.basename(paths)})\n\n")

                f.write("**Interpreta√ß√£o**: \n")
                f.write("- **Converg√™ncia entre rounds** indica comportamento reproduz√≠vel\n")
                f.write("- **Diverg√™ncias significativas** podem indicar efeitos de noisy neighbors\n")
                f.write("- **Padr√µes temporais consistentes** sugerem rela√ß√µes causais est√°veis\n\n")
            else:
                f.write("Time series consolidados n√£o foram gerados nesta execu√ß√£o.\n\n")

            # --- Se√ß√£o 4.3: Gr√°ficos de Correla√ß√£o Agregada ---
            f.write("## 4.3. Gr√°ficos de Correla√ß√£o Agregada\n\n")
            f.write("Estes grafos mostram as correla√ß√µes m√©dias entre os tenants, agregadas atrav√©s de todos os rounds e fases. As arestas representam a for√ßa da correla√ß√£o (positiva ou negativa) entre os pares de tenants.\n\n")
            
            aggregated_correlation_graphs = all_results.get('aggregated_correlation_graphs', {})
            if aggregated_correlation_graphs:
                for metric, path in aggregated_correlation_graphs.items():
                    metric_display = metric.replace("_", " ").title()
                    f.write(f"### {metric_display}\n")
                    f.write(f"![Grafo de Correla√ß√£o Agregada - {metric_display}](./correlation/{os.path.basename(path)})\n\n")
            else:
                f.write("Gr√°ficos de correla√ß√£o agregada n√£o foram gerados nesta execu√ß√£o.\n\n")


            # --- Se√ß√£o 5: Veredictos de Consenso ---
            f.write("## 5. Veredictos de Consenso\n\n")
            consensus = all_results.get('consensus', {})
            if consensus:
                f.write("Agrega√ß√£o dos resultados de todos os rounds para produzir um veredito final.\n\n")
                if consensus.get('noisy_tenants_consensus'):
                    f.write("### Tenants Barulhentos (Consenso)\n")
                    f.write("Tenants identificados como fontes de causalidade de forma consistente na maioria dos rounds. Veja `consensus_noisy_tenants.csv`.\n\n")
                if consensus.get('tenant_influence_ranking'):
                    f.write("### Ranking de Influ√™ncia de Tenants (Consenso)\n")
                    f.write("Ranking de tenants com base na sua influ√™ncia causal consolidada. Veja `tenant_influence_ranking.csv`.\n\n")
            else:
                f.write("An√°lise de consenso n√£o foi executada ou n√£o produziu resultados.\n\n")

            # --- Se√ß√£o de Sum√°rio ---
            f.write("## Sum√°rio Final\n\n")
            f.write("A an√°lise multi-round fornece insights sobre a estabilidade e reprodutibilidade dos resultados do experimento. Alta consist√™ncia sugere que as rela√ß√µes causais e comportamentos observados s√£o robustos. Baixa consist√™ncia pode indicar que o sistema exibe comportamento vari√°vel ou que os resultados s√£o sens√≠veis a condi√ß√µes iniciais, necessitando de investiga√ß√£o adicional.\n")

        self.logger.info(f"Relat√≥rio consolidado de an√°lise multi-round salvo em: {report_path}")

    def _load_causality_matrices(self, context: Dict[str, Any], rounds: List[str]) -> Dict[str, Any]:
        """
        Carrega as matrizes de causalidade (TE e Granger) para cada round.
        Primeiro, tenta carregar do contexto. Se n√£o encontrar, busca os arquivos CSV
        no diret√≥rio de sa√≠da do pipeline principal.
        """
        te_matrices_by_round = {}
        granger_matrices_by_round = {}
        
        # Prioriza o diret√≥rio de sa√≠da principal do contexto para maior robustez
        base_output_dir = context.get('output_dir')
        if not base_output_dir:
            # Fallback para o caso de o diret√≥rio do contexto n√£o estar dispon√≠vel
            base_output_dir = os.path.dirname(self.output_dir) if self.output_dir else None

        if not base_output_dir:
            self.logger.warning("N√£o foi poss√≠vel determinar o diret√≥rio base de sa√≠das. N√£o ser√° poss√≠vel carregar matrizes de causalidade.")
            return {}

        causality_output_dir = os.path.join(base_output_dir, 'plots', 'causality')
        
        self.logger.info(f"Procurando matrizes de causalidade em: {causality_output_dir}")

        for round_id in rounds:
            # Tenta carregar do contexto primeiro
            te_key = f'te_matrices_round_{round_id}'
            granger_key = f'granger_matrices_round_{round_id}'
            
            if te_key in context and granger_key in context:
                te_matrices_by_round[round_id] = context[te_key]
                granger_matrices_by_round[round_id] = context[granger_key]
                self.logger.info(f"Matrizes de causalidade para o round '{round_id}' carregadas do contexto.")
                continue

            # Se n√£o estiver no contexto, carregar dos arquivos
            self.logger.info(f"Matrizes para o round '{round_id}' n√£o encontradas no contexto. Tentando carregar de arquivos...")
            round_causality_path = os.path.join(causality_output_dir, round_id)
            
            if not os.path.isdir(round_causality_path):
                self.logger.warning(f"Diret√≥rio de causalidade para o round '{round_id}' n√£o encontrado em '{round_causality_path}'.")
                continue

            te_matrices = {}
            granger_matrices = {}
            
            try:
                for file_name in os.listdir(round_causality_path):
                    if file_name.startswith('te_matrix_') and file_name.endswith('.csv'):
                        metric = file_name.replace('te_matrix_', '').replace('.csv', '')
                        file_path = os.path.join(round_causality_path, file_name)
                        te_matrices[metric] = pd.read_csv(file_path, index_col=0)
                    elif file_name.startswith('granger_matrix_') and file_name.endswith('.csv'):
                        metric = file_name.replace('granger_matrix_', '').replace('.csv', '')
                        file_path = os.path.join(round_causality_path, file_name)
                        granger_matrices[metric] = pd.read_csv(file_path, index_col=0)
            except Exception as e:
                self.logger.error(f"Erro ao carregar arquivos de matriz do diret√≥rio {round_causality_path}: {e}")

            if te_matrices:
                te_matrices_by_round[round_id] = te_matrices
                self.logger.info(f"  - {len(te_matrices)} matrizes de Transfer Entropy carregadas para o round '{round_id}'.")
            if granger_matrices:
                granger_matrices_by_round[round_id] = granger_matrices
                self.logger.info(f"  - {len(granger_matrices)} matrizes de Granger carregadas para o round '{round_id}'.")

        return {
            'te_matrices_by_round': te_matrices_by_round,
            'granger_matrices_by_round': granger_matrices_by_round
        }


    def _extract_effect_sizes(self, context: Dict[str, Any], config: Dict[str, Any], df_filtered: pd.DataFrame, rounds: List[str]) -> pd.DataFrame:
        """
        Extrai tamanhos de efeito (effect sizes) para todas as combina√ß√µes relevantes.

        Args:
            context: Dicion√°rio de contexto do pipeline
            config: Dicion√°rio de configura√ß√£o
            df_filtered: DataFrame com dados j√° filtrados
            rounds: Lista de rounds para extra√ß√£o

        Returns:
            pd.DataFrame: DataFrame com todos os tamanhos de efeito extra√≠dos
        """
        self.logger.info("Extraindo tamanhos de efeito para an√°lise multi-round...")
        
        # Configura√ß√£o para extra√ß√£o de tamanhos de efeito
        effect_size_config = config.get('multi_round_analysis', {}).get('effect_size', {})
        baseline_phase = effect_size_config.get('baseline_phase', "1 - Baseline")
        
        # Obter configura√ß√µes de desempenho
        perf_config = config.get('multi_round_analysis', {}).get('performance', {})
        parallel = perf_config.get('parallel_processing', False)
        use_gpu = perf_config.get('gpu_acceleration', False)
        large_dataset_threshold = perf_config.get('large_dataset_threshold', 10000)
        use_cache = perf_config.get('use_cache', True)
        
        # Obter listas de m√©tricas e tenants
        metrics = context.get('selected_metrics', sorted(df_filtered['metric_name'].unique()))
        tenants = context.get('selected_tenants', sorted(df_filtered['tenant_id'].unique()))
        phases = sorted(df_filtered['experimental_phase'].unique())
        
        # Configurar cache
        cache_dir = os.path.join(self.output_dir, '_cache') if self.output_dir else None
        
        # Chamar a fun√ß√£o extract_effect_sizes com paraleliza√ß√£o se configurado
        try:
            effect_sizes_df = extract_effect_sizes(
                df_long=df_filtered,
                rounds=rounds,
                metrics=metrics,
                phases=phases,
                tenants=tenants,
                baseline_phase=baseline_phase,
                parallel=parallel,
                use_cache=use_cache,
                cache_dir=cache_dir,
                use_gpu=use_gpu,
                large_dataset_threshold=large_dataset_threshold
            )
            
            # Registrar m√©tricas para logar
            total_combinations = len(effect_sizes_df) if not effect_sizes_df.empty else 0
            self.logger.info(f"Extra√≠dos {total_combinations} tamanhos de efeito para {len(rounds)} rounds.")
            
            return effect_sizes_df
        except Exception as e:
            self.logger.error(f"Erro ao extrair tamanhos de efeito: {str(e)}")
            return pd.DataFrame()  # Retornar DataFrame vazio em caso de erro

    def _aggregate_effect_sizes(self, effect_sizes_df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """
        Agrega os tamanhos de efeito por m√©trica √ó fase √ó tenant atrav√©s dos rounds,
        calculando m√©dias, desvios padr√£o, intervalos de confian√ßa e p-valores combinados.
        
        Args:
            effect_sizes_df: DataFrame com tamanhos de efeito por round
            config: Configura√ß√£o da an√°lise
            
        Returns:
            DataFrame com estat√≠sticas agregadas
        """
        self.logger.info("Agregando tamanhos de efeito por m√©trica, fase e tenant...")
        
        if effect_sizes_df.empty:
            self.logger.warning("DataFrame de tamanhos de efeito vazio. Pulando agrega√ß√£o.")
            return pd.DataFrame()
        
        # Obt√©m par√¢metros de configura√ß√£o
        multi_round_config = config.get('multi_round_analysis', {})
        meta_config = multi_round_config.get('meta_analysis', {})
        
        # Define par√¢metros para agrega√ß√£o
        alpha = meta_config.get('alpha', 0.05)
        p_value_method = meta_config.get('p_value_combination', 'fisher')
        confidence_level = meta_config.get('confidence_level', 0.95)
        use_bootstrap = meta_config.get('use_bootstrap', True)
        n_bootstrap = meta_config.get('n_bootstrap', 1000)
        
        # Realiza agrega√ß√£o
        aggregated_df = aggregate_effect_sizes(
            effect_sizes_df=effect_sizes_df,
            alpha=alpha,
            p_value_method=p_value_method,
            confidence_level=confidence_level,
            use_bootstrap=use_bootstrap,
            n_bootstrap=n_bootstrap
        )
        
        if aggregated_df.empty:
            self.logger.warning("Nenhuma estat√≠stica agregada calculada.")
        else:
            self.logger.info(f"Agrega√ß√£o conclu√≠da: {aggregated_df.shape[0]} estat√≠sticas agregadas.")
            
            # Salva os resultados em CSV se houver diret√≥rio de sa√≠da
            if self.output_dir:
                aggregated_path = os.path.join(self.output_dir, 'aggregated_effects.csv')
                aggregated_df.to_csv(aggregated_path, index=False)
                self.logger.info(f"Estat√≠sticas agregadas salvas em: {aggregated_path}")
                
                # Gera um resumo das estat√≠sticas
                summary_path = os.path.join(self.output_dir, 'effect_size_summary.md')
                with open(summary_path, 'w') as f:
                    f.write("# Resumo da An√°lise de Tamanhos de Efeito\n\n")
                    f.write(f"Gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                    
                    # Resumo de estat√≠sticas gerais
                    f.write("## Estat√≠sticas Gerais\n\n")
                    f.write(f"- Total de combina√ß√µes analisadas: {aggregated_df.shape[0]}\n")
                    f.write(f"- Efeitos estatisticamente significativos: {aggregated_df['is_significant'].sum()}\n")
                    
                    # Resumo por magnitude de efeito
                    magnitude_counts = aggregated_df['effect_magnitude'].value_counts()
                    f.write("\n### Distribui√ß√£o por Magnitude de Efeito\n\n")
                    for magnitude, count in magnitude_counts.items():
                        f.write(f"- {str(magnitude).title()}: {count}\n")
                    
                    # Resumo por confiabilidade
                    reliability_counts = aggregated_df['reliability_category'].value_counts()
                    f.write("\n### Distribui√ß√£o por Confiabilidade\n\n")
                    for reliability, count in reliability_counts.items():
                        f.write(f"- {str(reliability).title()}: {count}\n")
                    
                    # Top 5 efeitos mais fortes e significativos
                    significant_df = aggregated_df[aggregated_df['is_significant']]
                    if not significant_df.empty:
                        top_effects = significant_df.sort_values('mean_effect_size', key=abs, ascending=False).head(5)
                        f.write("\n## Top 5 Efeitos Mais Fortes (Significativos)\n\n")
                        for _, row in top_effects.iterrows():
                            f.write(f"- **{row['experimental_phase']} em {row['tenant_id']} (m√©trica: {row['metric_name']})**\n")
                            f.write(f"  - Tamanho de efeito: {row['mean_effect_size']:.3f} (IC95%: {row['ci_lower']:.3f} a {row['ci_upper']:.3f})\n")
                            f.write(f"  - p-valor combinado: {row['combined_p_value']:.6f} ({row['rounds_count']} rounds)\n")
                            f.write(f"  - Confiabilidade: {row['reliability_category'].title()}\n")
        
        return aggregated_df

    def _extract_phase_correlations(self, context: Dict[str, Any], config: Dict[str, Any], df_long: pd.DataFrame, rounds: List[str]) -> pd.DataFrame:
        """
        Extrai as correla√ß√µes intra-fase entre tenants para cada 
        combina√ß√£o de m√©trica √ó fase √ó round.
        
        Args:
            context: Contexto da an√°lise
            config: Configura√ß√£o da an√°lise
            df_long: DataFrame em formato longo
            rounds: Lista de rounds a analisar
            
        Returns:
            DataFrame com correla√ß√µes intra-fase
        """
        self.logger.info("Extraindo correla√ß√µes intra-fase para todas as combina√ß√µes...")
        
        # Obt√©m par√¢metros de configura√ß√£o
        multi_round_config = config.get('multi_round_analysis', {})
        correlation_config = multi_round_config.get('correlation', {})
        
        # Define par√¢metros para correla√ß√£o
        method = correlation_config.get('method', 'pearson')
        min_periods = correlation_config.get('min_periods', 3)
        
        # Obt√©m m√©tricas, fases e tenants
        metrics = context.get('selected_metrics', sorted(df_long['metric_name'].unique()))
        phases = sorted(df_long['experimental_phase'].unique())
        tenants = context.get('selected_tenants', sorted(df_long['tenant_id'].unique()))
        
        # Configura√ß√µes de cache e paralelismo
        perf_config = multi_round_config.get('performance', {})
        use_cache = perf_config.get('use_cache', True)
        parallel = perf_config.get('parallel_processing', False)
        cache_dir = os.path.join(self.output_dir, 'cache') if self.output_dir else None
        
        # Extrai correla√ß√µes intra-fase
        correlations_df = extract_phase_correlations(
            df_long=df_long,
            rounds=rounds,
            metrics=metrics,
            phases=phases,
            tenants=tenants,
            method=method,
            min_periods=min_periods,
            use_cache=use_cache,
            parallel=parallel,
            cache_dir=cache_dir
        )
        
        if correlations_df.empty:
            self.logger.warning("Nenhuma correla√ß√£o intra-fase extra√≠da. Verifique os dados e par√¢metros.")
        else:
            self.logger.info(f"Extra√ß√£o conclu√≠da: {correlations_df.shape[0]} correla√ß√µes intra-fase calculadas.")
            
            # Salva os resultados em CSV se houver diret√≥rio de sa√≠da
            if self.output_dir:
                correlations_path = os.path.join(self.output_dir, 'phase_correlations.csv')
                correlations_df.to_csv(correlations_path, index=False)
                self.logger.info(f"Correla√ß√µes intra-fase salvas em: {correlations_path}")
                
                # Analisa a estabilidade das correla√ß√µes entre rounds
                min_rounds = correlation_config.get('min_stable_rounds', 2)
                correlation_threshold = correlation_config.get('significance_threshold', 0.5)
                
                stability_results = analyze_correlation_stability(
                    phase_correlations_df=correlations_df,
                    min_rounds=min_rounds,
                    correlation_threshold=correlation_threshold
                )
                
                if stability_results:
                    # Gera um resumo da estabilidade das correla√ß√µes
                    stability_path = os.path.join(self.output_dir, 'correlation_stability_summary.md')
                    with open(stability_path, 'w') as f:
                        f.write("# Resumo da Estabilidade das Correla√ß√µes Intra-Fase\n\n")
                        f.write(f"Gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                        
                        # Resumo de estat√≠sticas gerais
                        f.write("## Estat√≠sticas Gerais\n\n")
                        summary = stability_results.get('summary', {})
                        f.write(f"- Total de pares analisados: {summary.get('total_pairs', 0)}\n")
                        f.write(f"- Pares com correla√ß√£o est√°vel: {summary.get('stable_pairs', 0)}\n")
                        f.write(f"- Pares com correla√ß√£o inst√°vel: {summary.get('unstable_pairs', 0)}\n")
                        
                        # Correla√ß√µes est√°veis por m√©trica e fase
                        stable_correlations = stability_results.get('stable_correlations', {})
                        if stable_correlations:
                            f.write("\n## Correla√ß√µes Est√°veis por M√©trica e Fase\n\n")
                            for (metric, phase), correlations in stable_correlations.items():
                                f.write(f"### {metric} - {phase}\n\n")
                                # Ordena por correla√ß√£o m√©dia (valor absoluto) decrescente
                                correlations.sort(key=lambda x: abs(x['mean_correlation']), reverse=True)
                                for corr_info in correlations:
                                    tenant_pair = corr_info['tenant_pair']
                                    mean_corr = corr_info['mean_correlation']
                                    std_corr = corr_info['std_correlation']
                                    variability = corr_info['variability']
                                    rounds_count = corr_info['round_count']
                                    
                                    f.write(f"- **{tenant_pair}**: {mean_corr:.3f} ¬± {std_corr:.3f} ({variability} variability, {rounds_count} rounds)\n")
                                f.write("\n")
                        
                        # Variabilidade por m√©trica e fase
                        variability = stability_results.get('correlation_variability', [])
                        if variability:
                            f.write("\n## Variabilidade da Correla√ß√£o por M√©trica e Fase\n\n")
                            f.write("| M√©trica | Fase | Desvio Padr√£o M√©dio | CV M√©dio | % Pares Est√°veis |\n")
                            f.write("|---------|------|---------------------|----------|------------------|\n")
                            for var_info in variability:
                                metric = var_info['metric_name']
                                phase = var_info['experimental_phase']
                                mean_std = var_info['mean_std']
                                mean_cv = var_info['mean_cv']
                                stable_ratio = var_info['stable_ratio'] * 100
                                
                                f.write(f"| {metric} | {phase} | {mean_std:.3f} | {mean_cv:.3f} | {stable_ratio:.1f}% |\n")
                    
                    self.logger.info(f"Resumo de estabilidade das correla√ß√µes salvo em: {stability_path}")
        
        return correlations_df

def analyze_round_consistency(df_long: pd.DataFrame, metrics: List[str], tenants: List[str], output_dir: Optional[str] = None) -> Dict[str, Any]:
    """
    Analisa a consist√™ncia das m√©tricas entre os rounds usando o Coeficiente de Varia√ß√£o (CV)
    e testes estat√≠sticos de Friedman para diferen√ßas entre rounds.
    
    Args:
        df_long: DataFrame em formato longo
        metrics: Lista de m√©tricas para an√°lise
        tenants: Lista de tenants para an√°lise
        output_dir: Diret√≥rio para salvar os resultados
        
    Returns:
        Dict[str, Any]: Dicion√°rio com resultados da an√°lise
    """
    logger.info("Analisando a consist√™ncia das m√©tricas entre os rounds...")
    
    results = {
        'cv_by_metric_tenant': {},
        'friedman_tests': {},
        'round_outliers': {}
    }
    
    # Verificar se h√° mais de um round para an√°lise
    rounds = sorted(df_long['round_id'].unique())
    if len(rounds) < 2:
        logger.warning("Pelo menos dois rounds s√£o necess√°rios para an√°lise de consist√™ncia. Pulando.")
        return results
    
    # Calcular CV para cada combina√ß√£o de m√©trica x tenant x fase
    cv_results = []
    
    for metric in metrics:
        for tenant in tenants:
            df_filtered = df_long[(df_long['metric_name'] == metric) & 
                                 (df_long['tenant_id'] == tenant)]
            
            if df_filtered.empty:
                continue
            
            for phase in df_filtered['experimental_phase'].unique():
                phase_data = df_filtered[df_filtered['experimental_phase'] == phase]
                
                # Agregar por round para obter um valor m√©dio por round
                agg_by_round = phase_data.groupby('round_id')['metric_value'].mean()
                
                if len(agg_by_round) > 1:  # S√≥ podemos calcular CV se tivermos mais de um valor
                    mean_val = agg_by_round.mean()
                    std_val = agg_by_round.std()
                    cv = (std_val / mean_val) if mean_val != 0 else float('inf')
                    
                    cv_results.append({
                        'metric_name': metric,
                        'tenant_id': tenant,
                        'experimental_phase': phase,
                        'mean': mean_val,
                        'std': std_val,
                        'cv': cv,
                        'rounds_count': len(agg_by_round),
                        'rounds': ','.join(agg_by_round.index.tolist())
                    })
    
    # Converter para DataFrame
    if cv_results:
        cv_df = pd.DataFrame(cv_results)
        results['cv_by_metric_tenant'] = cv_df
        
        # Salvar resultados
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            cv_path = os.path.join(output_dir, 'round_consistency_cv.csv')
            cv_df.to_csv(cv_path, index=False)
            logger.info(f"Resultados de CV salvos em: {cv_path}")
            
            # Gerar visualiza√ß√£o de heatmap para CV
            try:
                # Preparar dados para heatmap
                pivot_df = cv_df.pivot_table(
                    index='tenant_id', 
                    columns='metric_name', 
                    values='cv', 
                    aggfunc='mean'
                )
                
                # Gerar heatmap
                plt.figure(figsize=(10, 8))
                sns.heatmap(
                    pivot_df, 
                    annot=True, 
                    fmt=".2f", 
                    cmap='viridis_r',  # Invertido para que valores baixos (mais consistentes) sejam verde escuro
                    linewidths=0.5
                )
                plt.title('Coeficiente de Varia√ß√£o (CV) por Tenant e M√©trica\nValores mais baixos indicam maior consist√™ncia entre rounds')
                plt.tight_layout()
                
                heatmap_path = os.path.join(output_dir, 'cv_heatmap_by_tenant_metric.png')
                plt.savefig(heatmap_path, dpi=300)
                plt.close()
                
                results['heatmap_path'] = heatmap_path
                logger.info(f"Heatmap de CV salvo em: {heatmap_path}")
            
            except Exception as e:
                logger.error(f"Erro ao gerar heatmap de CV: {str(e)}")
    
    # Realizar testes de Friedman para cada m√©trica e tenant
    # Apenas se tivermos pelo menos 3 rounds (requisito do teste de Friedman)
    if len(rounds) >= 3:
        friedman_results = []
        
        for metric in metrics:
            for tenant in tenants:
                # Preparar dados para o teste de Friedman
                # Precisa de uma matriz com fases nas linhas e rounds nas colunas
                phases_data = {}
                
                for phase in df_long['experimental_phase'].unique():
                    phase_by_round = {}
                    
                    for round_id in rounds:
                        data = df_long[
                            (df_long['metric_name'] == metric) & 
                            (df_long['tenant_id'] == tenant) &
                            (df_long['experimental_phase'] == phase) &
                            (df_long['round_id'] == round_id)
                        ]
                        
                        if not data.empty:
                            phase_by_round[round_id] = data['metric_value'].mean()
                    
                    if len(phase_by_round) == len(rounds):  # Temos dados para todos os rounds
                        phases_data[phase] = [phase_by_round[r] for r in rounds]
                
                if phases_data:
                    try:
                        phases_df = pd.DataFrame(phases_data).T  # Transpor para ter fases nas linhas
                        
                        # Realizar teste de Friedman
                        friedman_stat, p_value = stats.friedmanchisquare(*[phases_df[col] for col in phases_df.columns])
                        
                        friedman_results.append({
                            'metric_name': metric,
                            'tenant_id': tenant,
                            'friedman_statistic': friedman_stat,
                            'p_value': p_value,
                            'significant': p_value < 0.05,
                            'phases_count': len(phases_data)
                        })
                    except Exception as e:
                        logger.warning(f"Erro no teste de Friedman para {metric}, {tenant}: {str(e)}")
        
        if friedman_results:
            friedman_df = pd.DataFrame(friedman_results)
            results['friedman_tests'] = friedman_df
            
            # Salvar resultados
            if output_dir:
                friedman_path = os.path.join(output_dir, 'friedman_tests.csv')
                friedman_df.to_csv(friedman_path, index=False)
                logger.info(f"Resultados dos testes de Friedman salvos em: {friedman_path}")
    
    # Detectar rounds outliers com base nos CVs
    if 'cv_by_metric_tenant' in results and not results['cv_by_metric_tenant'].empty:
        cv_df = results['cv_by_metric_tenant']
        
        # Para cada m√©trica e fase, identificar quais rounds s√£o outliers
        outliers_by_metric = {}
        
        for metric in metrics:
            metric_cv = cv_df[cv_df['metric_name'] == metric]
            if metric_cv.empty:
                continue
                
            # Calcular m√©dia global do CV para esta m√©trica
            mean_cv = metric_cv['cv'].mean()
            std_cv = metric_cv['cv'].std()
            
            for phase in metric_cv['experimental_phase'].unique():
                phase_cv = metric_cv[metric_cv['experimental_phase'] == phase]
                
                # Identificar valores de CV muito altos (outliers)
                outlier_threshold = mean_cv + 2 * std_cv  # 2 desvios padr√£o acima da m√©dia
                
                outliers = phase_cv[phase_cv['cv'] > outlier_threshold]
                if not outliers.empty:
                    for _, row in outliers.iterrows():
                        metric_key = f"{metric}_{row['tenant_id']}"
                        if metric_key not in outliers_by_metric:
                            outliers_by_metric[metric_key] = []
                            
                        outliers_by_metric[metric_key].append({
                            'phase': row['experimental_phase'],
                            'cv': row['cv'],
                            'threshold': outlier_threshold,
                            'rounds': row['rounds']
                        })
        
        results['round_outliers'] = outliers_by_metric
    
    logger.info("An√°lise de consist√™ncia entre rounds conclu√≠da.")
    return results

def analyze_behavioral_divergence(df_long: pd.DataFrame, metrics: List[str], tenants: List[str], output_dir: Optional[str] = None) -> Dict[str, Any]:
    """
    Analisa a diverg√™ncia comportamental entre rounds usando a Diverg√™ncia de Kullback-Leibniz (KL).
    Identifica rounds com comportamento an√¥malo e mede a estabilidade comportamental dos tenants.
    
    Args:
        df_long: DataFrame em formato longo
        metrics: Lista de m√©tricas para an√°lise
        tenants: Lista de tenants para an√°lise
        output_dir: Diret√≥rio para salvar os resultados
        
    Returns:
        Dict[str, Any]: Dicion√°rio com resultados da an√°lise
    """
    logger.info("Analisando a diverg√™ncia comportamental entre rounds...")
    
    results = {
        'kl_divergence': [],
        'round_distances': {},
        'tenant_stability_scores': []
    }
    
    # Verificar se h√° mais de um round para an√°lise
    rounds = sorted(df_long['round_id'].unique())
    if len(rounds) < 2:
        logger.warning("Pelo menos dois rounds s√£o necess√°rios para an√°lise de diverg√™ncia. Pulando.")
        return results
    
    # Para cada m√©trica e tenant, calcular diverg√™ncia KL entre distribui√ß√µes por round
    for metric in metrics:
        metric_results = {'metric_name': metric, 'tenant_divergences': {}}
        
        for tenant in tenants:
            tenant_data = df_long[(df_long['metric_name'] == metric) & 
                                 (df_long['tenant_id'] == tenant)]
            
            if tenant_data.empty:
                continue
            
            round_distributions = {}
            
            # Construir distribui√ß√µes emp√≠ricas para cada round
            for round_id in rounds:
                round_data = tenant_data[tenant_data['round_id'] == round_id]
                
                if round_data.empty:
                    continue
                
                # Usar histograma para estimar a distribui√ß√£o
                hist, bin_edges = np.histogram(round_data['metric_value'], bins=20, density=True)
                
                # Suavizar zeros para evitar diverg√™ncia infinita
                hist = np.where(hist == 0, 1e-10, hist)
                # Normalizar para garantir que soma a 1
                hist = hist / np.sum(hist)
                
                round_distributions[round_id] = hist
            
            # Calcular matriz de diverg√™ncia KL entre todos os pares de rounds
            if len(round_distributions) >= 2:
                divergence_matrix = np.zeros((len(round_distributions), len(round_distributions)))
                round_ids = list(round_distributions.keys())
                
                for i, round1 in enumerate(round_ids):
                    for j, round2 in enumerate(round_ids):
                        if i == j:
                            divergence_matrix[i, j] = 0
                        else:
                            # Calcular diverg√™ncia KL sim√©trica
                            p = round_distributions[round1]
                            q = round_distributions[round2]
                            
                            # Calcular KL em ambas as dire√ß√µes e tomar a m√©dia
                            # KL(P||Q) = sum_i P_i * log(P_i/Q_i)
                            kl_pq = np.sum(p * np.log(p / q))
                            kl_qp = np.sum(q * np.log(q / p))
                            
                            # Diverg√™ncia sim√©trica
                            sym_kl = (kl_pq + kl_qp) / 2
                            divergence_matrix[i, j] = sym_kl
                
                # Armazenar resultados
                metric_results['tenant_divergences'][tenant] = {
                    'round_ids': round_ids,
                    'divergence_matrix': divergence_matrix.tolist()
                }
                
                # Calcular estabilidade do tenant com base na diverg√™ncia m√©dia
                avg_divergence = np.mean(divergence_matrix[np.triu_indices_from(divergence_matrix, k=1)])
                
                results['kl_divergence'].append({
                    'metric_name': metric,
                    'tenant_id': tenant,
                    'mean_divergence': avg_divergence,
                    'stability_score': 1 / (1 + avg_divergence)  # Converter diverg√™ncia para score de estabilidade [0,1]
                })
        
        results['round_distances'][metric] = metric_results
    
    # Calcular scores de estabilidade por tenant
    if results['kl_divergence']:
        stability_df = pd.DataFrame(results['kl_divergence'])
        
        # Calcular estabilidade m√©dia por tenant entre todas as m√©tricas
        tenant_stability = stability_df.groupby('tenant_id')['stability_score'].mean().reset_index()
        tenant_stability = tenant_stability.sort_values('stability_score', ascending=False)
        
        results['tenant_stability_scores'] = tenant_stability.to_dict('records')
        
        # Salvar resultados
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
            # Salvar scores de estabilidade
            stability_path = os.path.join(output_dir, 'tenant_stability_scores.csv')
            tenant_stability.to_csv(stability_path, index=False)
            logger.info(f"Scores de estabilidade de tenant salvos em: {stability_path}")
            
            # Salvar diverg√™ncias KL
            kl_path = os.path.join(output_dir, 'kl_divergence.csv')
            pd.DataFrame(results['kl_divergence']).to_csv(kl_path, index=False)
            logger.info(f"Diverg√™ncias KL salvas em: {kl_path}")
            
            # Gerar visualiza√ß√£o de barras para estabilidade de tenant
            try:
                plt.figure(figsize=(12, 6))
                bars = plt.bar(tenant_stability['tenant_id'], tenant_stability['stability_score'], alpha=0.7)
                
                # Adicionar valores nas barras
                for bar in bars:
                    height = bar.get_height()
                    plt.text(
                        bar.get_x() + bar.get_width()/2.,
                        height + 0.01,
                        f"{height:.2f}",
                        ha='center',
                        va='bottom',
                        fontsize=9
                    )
                
                plt.title('Scores de Estabilidade por Tenant\nValores mais altos indicam comportamento mais est√°vel entre rounds')
                plt.ylim(0, max(tenant_stability['stability_score']) * 1.1)  # Ajustar limites do eixo y
                plt.ylabel('Score de Estabilidade')
                plt.tight_layout()
                
                stability_plot_path = os.path.join(output_dir, 'tenant_stability_scores.png')
                plt.savefig(stability_plot_path, dpi=300)
                plt.close()
                
                results['stability_plot_path'] = stability_plot_path
                logger.info(f"Plot de estabilidade de tenant salvo em: {stability_plot_path}")
            
            except Exception as e:
                logger.error(f"Erro ao gerar plot de estabilidade de tenant: {str(e)}")
    
    logger.info("An√°lise de diverg√™ncia comportamental conclu√≠da.")
    return results

def aggregate_round_consensus(df_long: pd.DataFrame, te_matrices_by_round: Dict[str, Dict[str, pd.DataFrame]], consistency_results: Dict[str, Any], output_dir: Optional[str] = None) -> Dict[str, Any]:
    """
    Agrega os resultados de m√∫ltiplos rounds para gerar um consenso final.
    
    Args:
        df_long: DataFrame em formato longo
        te_matrices_by_round: Dicion√°rio de matrizes de Transfer Entropy por round
        consistency_results: Resultados da an√°lise de consist√™ncia
        output_dir: Diret√≥rio para salvar os resultados
        
    Returns:
        Dict[str, Any]: Dicion√°rio com resultados do consenso
    """
    logger.info("Agregando consenso entre rounds...")
    
    results = {
        'noisy_tenants_consensus': [],
        'tenant_influence_ranking': []
    }
    
    # Verificar se temos matrizes de TE suficientes
    rounds = list(te_matrices_by_round.keys())
    if len(rounds) < 2:
        logger.warning("Pelo menos dois rounds com matrizes TE s√£o necess√°rios para agrega√ß√£o de consenso. Pulando.")
        return results
    
    # M√©tricas dispon√≠veis (interse√ß√£o de todas as matrizes)
    available_metrics = set(te_matrices_by_round[rounds[0]].keys())
    for round_id in rounds[1:]:
        available_metrics &= set(te_matrices_by_round[round_id].keys())
    
    if not available_metrics:
        logger.warning("Nenhuma m√©trica comum encontrada em todas as matrizes TE. Pulando agrega√ß√£o de consenso.")
        return results
    
    # Para cada m√©trica, agregar matrizes TE de todos os rounds
    aggregated_te = {}
    
    for metric in available_metrics:
        # Verificar se todas as matrizes t√™m as mesmas dimens√µes e tenants
        matrices = [te_matrices_by_round[r][metric] for r in rounds]
        
        # Verificar se todas as matrizes t√™m os mesmos tenants
        tenant_sets = [set(m.index) for m in matrices]
        common_tenants = tenant_sets[0]
        for tenant_set in tenant_sets[1:]:
                       common_tenants &= tenant_set
        
        if not common_tenants:
            logger.warning(f"Nenhum tenant comum encontrado para a m√©trica {metric}. Pulando.")
            continue
        
        # Converter para lista ordenada para consist√™ncia
        common_tenants = sorted(common_tenants)
        
        # Inicializar matriz agregada
        aggregated_matrix = pd.DataFrame(0, index=common_tenants, columns=common_tenants)
        
        # Calcular m√©dia ponderada baseada na consist√™ncia (se dispon√≠vel)
        if consistency_results and 'cv_by_metric_tenant' in consistency_results:
            # Tentar obter pesos baseados no CV (menor CV = maior peso)
            try:
                cv_df = consistency_results['cv_by_metric_tenant']
                if isinstance(cv_df, pd.DataFrame) and not cv_df.empty and 'cv' in cv_df.columns:
                    # Filtrar para a m√©trica atual
                    metric_cv = cv_df[cv_df['metric_name'] == metric]
                    
                    if not metric_cv.empty:
                        # Calcular pesos por round (inverso do CV m√©dio)
                        round_weights = {}
                        for round_id in rounds:
                            round_data = df_long[df_long['round_id'] == round_id]
                            if round_data.empty:
                                round_weights[round_id] = 1.0  # Peso padr√£o
                                continue
                            
                            # Filtrar dados para a m√©trica atual
                            round_metric_data = round_data[round_data['metric_name'] == metric]
                            if round_metric_data.empty:
                                round_weights[round_id] = 1.0  # Peso padr√£o
                                continue
                            
                            # Calcular CV m√©dio para este round e m√©trica
                            tenants = round_metric_data['tenant_id'].unique()
                            tenant_cvs = []
                            
                            for tenant in tenants:
                                tenant_rows = metric_cv[metric_cv['tenant_id'] == tenant]
                                if not tenant_rows.empty:
                                    tenant_cvs.append(tenant_rows['cv'].mean())
                            
                            if tenant_cvs:
                                mean_cv = np.mean(tenant_cvs)
                                # Peso = 1 / (1 + CV) para dar mais peso a rounds com menor variabilidade
                                round_weights[round_id] = 1.0 / (1.0 + mean_cv)
                            else:
                                round_weights[round_id] = 1.0  # Peso padr√£o
                        
                        # Normalizar pesos para somar 1
                        total_weight = sum(round_weights.values())
                        if total_weight > 0:
                            round_weights = {r: w / total_weight for r, w in round_weights.items()}
                        
                        # Aplicar pesos na agrega√ß√£o
                        for i, round_id in enumerate(rounds):
                            weight = round_weights.get(round_id, 1.0 / len(rounds))
                            matrix = matrices[i]
                            # Filtrar para os tenants comuns
                            filtered_matrix = matrix.loc[common_tenants, common_tenants]
                            aggregated_matrix += filtered_matrix * weight
                    else:
                        # Se n√£o houver dados de CV, usar m√©dia simples
                        for matrix in matrices:
                            filtered_matrix = matrix.loc[common_tenants, common_tenants]
                            aggregated_matrix += filtered_matrix / len(matrices)
            except Exception as e:
                logger.error(f"Erro ao calcular pesos para agrega√ß√£o de TE: {str(e)}")
                # Fallback para m√©dia simples
                for matrix in matrices:
                    filtered_matrix = matrix.loc[common_tenants, common_tenants]
                    aggregated_matrix += filtered_matrix / len(matrices)
        else:
            # Se n√£o houver dados de CV, usar m√©dia simples
            for matrix in matrices:
                filtered_matrix = matrix.loc[common_tenants, common_tenants]
                aggregated_matrix += filtered_matrix / len(matrices)
        
        aggregated_te[metric] = aggregated_matrix
    
        # Identificar tenants "barulhentos" (causam impacto em outros)
        noisy_tenants = []
        
        # Calcular "outflow causal" (soma da for√ßa causal que sai de cada tenant)
        for tenant in common_tenants:
            outflow = aggregated_matrix.loc[tenant].sum() - aggregated_matrix.loc[tenant, tenant]
            
            # Calcular estat√≠sticas para o ranking
            mean_te = outflow / (len(common_tenants) - 1) if len(common_tenants) > 1 else 0
            
            # Considerar como "barulhento" se o valor m√©dio de TE for maior que um limite
            te_threshold = 0.1  # Ajustar conforme necess√°rio
            
            if mean_te > te_threshold:
                noisy_tenants.append({
                    'metric_name': metric,
                    'tenant_id': tenant,
                    'outflow_causality': float(outflow),
                    'mean_te': float(mean_te),
                    'affected_tenants': len([t for t in common_tenants if aggregated_matrix.loc[tenant, t] > te_threshold and t != tenant])
                })
            
            # Adicionar ao ranking geral
            results['tenant_influence_ranking'].append({
                'metric_name': metric,
                'tenant_id': tenant,
                'influence_score': float(outflow),
                'mean_outgoing_te': float(mean_te)
            })
        
        # Adicionar tenants barulhentos ao consenso
        noisy_tenants.sort(key=lambda x: x['outflow_causality'], reverse=True)
        results['noisy_tenants_consensus'].extend(noisy_tenants)
    
    # Salvar resultados do consenso
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
        # Salvar matrizes TE agregadas
        te_dir = os.path.join(output_dir, 'aggregated_te')
        os.makedirs(te_dir, exist_ok=True)
        
        for metric, matrix in aggregated_te.items():
            matrix_path = os.path.join(te_dir, f'aggregated_te_matrix_{metric}.csv')
            matrix.to_csv(matrix_path)
            logger.info(f"Matriz TE agregada para {metric} salva em: {matrix_path}")
        
        # Salvar ranking de influ√™ncia
        if results['tenant_influence_ranking']:
            ranking_df = pd.DataFrame(results['tenant_influence_ranking'])
            ranking_path = os.path.join(output_dir, 'tenant_influence_ranking.csv')
            ranking_df.to_csv(ranking_path, index=False)
            logger.info(f"Ranking de influ√™ncia de tenants salvo em: {ranking_path}")
        
        # Salvar consenso de tenants barulhentos
        if results['noisy_tenants_consensus']:
            consensus_df = pd.DataFrame(results['noisy_tenants_consensus'])
            consensus_path = os.path.join(output_dir, 'consensus_noisy_tenants.csv')
            consensus_df.to_csv(consensus_path, index=False)
            logger.info(f"Consenso de tenants barulhentos salvo em: {consensus_path}")
    
    logger.info(f"Agrega√ß√£o de consenso conclu√≠da. {len(results['noisy_tenants_consensus'])} tenants barulhentos identificados.")
    return results

def analyze_causality_robustness(te_matrices_by_round, granger_matrices_by_round, output_dir):
    """
    Analyze the robustness of causal relationships across different experimental rounds.
    
    Args:
        te_matrices_by_round: Dictionary of Transfer Entropy matrices by round
        granger_matrices_by_round: Dictionary of Granger Causality matrices by round
        output_dir: Directory to save the output
    
    Returns:
        Dictionary with analysis results
    """
    import numpy as np
    import pandas as pd
    import os
    import matplotlib.pyplot as plt
    import seaborn as sns
    from collections import defaultdict
    
    # Create output directory for robustness analysis
    robustness_dir = os.path.join(output_dir, "causality_robustness")
    os.makedirs(robustness_dir, exist_ok=True)
    
    # Initialize results dictionary
    results = {
        "robust_causal_relationships": defaultdict(dict),
        "metrics_with_consistent_causality": [],
        "cv_te_matrices": {}
    }
    
    # Get all metrics and rounds
    all_metrics = set()
    all_rounds = sorted(list(te_matrices_by_round.keys()))
    
    for round_id in all_rounds:
        for metric in te_matrices_by_round[round_id].keys():
            all_metrics.add(metric)
    
    all_metrics = sorted(list(all_metrics))
    
    # Analyze each metric
    for metric in all_metrics:
        metric_te_matrices = {}
        
        # Collect all matrices for this metric across rounds
        for round_id in all_rounds:
            if metric in te_matrices_by_round[round_id]:
                metric_te_matrices[round_id] = te_matrices_by_round[round_id][metric]
        
        if len(metric_te_matrices) < 2:
            continue  # Need at least 2 rounds for robustness analysis
        
        # Calculate coefficient of variation (CV) for each causal pair
        tenants = None
        for round_id, matrices in metric_te_matrices.items():
            for phase, matrix in matrices.items():
                if matrix is not None and hasattr(matrix, "index") and len(matrix) > 0:
                    tenants = matrix.index
                    break
            if tenants is not None:
                break
        
        if tenants is None:
            continue
        
        # Calculate CV for TE values across rounds for each phase and pair
        cv_matrices = {}
        for phase in set().union(*[set(matrices.keys()) for matrices in metric_te_matrices.values()]):
            # Collect matrices for this phase across rounds
            phase_matrices = []
            for round_id in all_rounds:
                if round_id in metric_te_matrices and phase in metric_te_matrices[round_id]:
                    matrix = metric_te_matrices[round_id][phase]
                    if matrix is not None and len(matrix) > 0:
                        phase_matrices.append(matrix)
            
            if len(phase_matrices) < 2:
                continue
            
            # Stack values into a 3D array for calculation
            # First ensure all matrices have the same shape
            if len(phase_matrices) >= 2:
                # Create a list of arrays
                matrix_values = [m.values for m in phase_matrices]
                
                # Stack along a new axis (at the end)
                stacked_matrices = np.stack(matrix_values, axis=0)
                
                # Log the shape for debugging
                logging.info(f"Stacked matrices shape for phase {phase}: {stacked_matrices.shape}")
                
                # Calculate mean and standard deviation across rounds (axis=0)
                mean_matrix = np.nanmean(stacked_matrices, axis=0)
                std_matrix = np.nanstd(stacked_matrices, axis=0)
                
                # Log the shape of the mean and std matrices
                logging.info(f"Mean matrix shape: {mean_matrix.shape}, Std matrix shape: {std_matrix.shape}")
            
            # Calculate CV (coefficient of variation)
            with np.errstate(divide='ignore', invalid='ignore'):
                cv_matrix = np.abs(std_matrix / mean_matrix)
            
            # Create a DataFrame for the CV matrix
            # Ensure cv_matrix has the correct dimensions
            # Handle the case where we have 1D arrays instead of 2D matrices
            if len(cv_matrix.shape) == 1:
                # We have a 1D array, convert it to a proper matrix
                logging.warning(f"CV matrix is 1D with shape {cv_matrix.shape}, expected 2D. Reshaping...")
                
                # Create a properly sized matrix filled with NaNs
                cv_matrix_fixed = np.full((len(tenants), len(tenants)), np.nan)
                
                # Determine if we have enough values to fill a row or column
                if len(cv_matrix) == len(tenants):
                    # Assume it's diagonal data - just put values in the diagonal
                    for i in range(len(tenants)):
                        cv_matrix_fixed[i, i] = cv_matrix[i]
                else:
                    # Just put the values in the first elements
                    flat_idx = 0
                    for i in range(len(tenants)):
                        for j in range(len(tenants)):
                            if flat_idx < len(cv_matrix):
                                cv_matrix_fixed[i, j] = cv_matrix[flat_idx]
                                flat_idx += 1
                            else:
                                break
                
                cv_matrix = cv_matrix_fixed
            elif cv_matrix.shape != (len(tenants), len(tenants)):
                # We have a 2D matrix but wrong dimensions
                logging.warning(f"CV matrix has shape {cv_matrix.shape}, expected {(len(tenants), len(tenants))}. Reshaping...")
                
                # Create a properly sized matrix filled with NaNs
                cv_matrix_fixed = np.full((len(tenants), len(tenants)), np.nan)
                
                # Copy the data we have, respecting dimension limits
                rows = min(cv_matrix.shape[0], len(tenants))
                cols = min(cv_matrix.shape[1], len(tenants)) if len(cv_matrix.shape) > 1 else 1
                
                # Copy the available data
                cv_matrix_fixed[:rows, :cols] = cv_matrix[:rows, :cols]
                cv_matrix = cv_matrix_fixed
            
            cv_df = pd.DataFrame(cv_matrix, index=tenants, columns=tenants)
            cv_matrices[phase] = cv_df
            
            # Identificar robustez das rela√ß√µes causais (baixo CV)
            threshold = 0.3  # Threshold para CV considerar uma rela√ß√£o robusta
            for source in tenants:
                for target in tenants:
                    if source != target:
                        cv_value = cv_df.loc[source, target]
                        if not np.isnan(cv_value) and cv_value < threshold:
                            # Verificar se a TE m√©dia √© significativa
                            avg_te = mean_matrix[list(tenants).index(source), list(tenants).index(target)]
                            if avg_te > 0.05:  # Threshold para TE significativa
                                if phase not in results["robust_causal_relationships"][metric]:
                                    results["robust_causal_relationships"][metric][phase] = []
                                results["robust_causal_relationships"][metric][phase].append({
                                    "source": source, 
                                    "target": target, 
                                    "cv": cv_value,
                                    "avg_te": avg_te
                                })
        
        # Store CV matrices for this metric
        results["cv_te_matrices"][metric] = cv_matrices
        
        # Check if this metric has robust relationships
        has_robust = False
        for phase in results["robust_causal_relationships"].get(metric, {}).keys():
            if results["robust_causal_relationships"][metric][phase]:
                has_robust = True
                break
        
        if has_robust:
            results["metrics_with_consistent_causality"].append(metric)
    
    # Generate visualizations for CV matrices
    for metric, cv_matrices in results["cv_te_matrices"].items():
        for phase, cv_matrix in cv_matrices.items():
            # Plot the CV matrix as a heatmap
            fig, ax = plt.subplots(figsize=(10, 8))
            mask = np.eye(len(cv_matrix))  # Mask diagonal
            sns.heatmap(cv_matrix, annot=True, cmap="YlOrRd_r", fmt=".2f", 
                        mask=mask, cbar_kws={"label": "Coefficient of Variation"}, ax=ax)
            ax.set_title(f"Robustez de Causalidade (CV) - {metric} - {phase}")
            ax.set_xlabel("Alvo")
            ax.set_ylabel("Fonte")
            
            # Save the figure
            output_path = os.path.join(robustness_dir, f"cv_te_matrix_{metric}_{phase.replace(' ', '_')}.png")
            fig.savefig(output_path, bbox_inches="tight", dpi=300)
            plt.close(fig)
    
    return results
